{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xAUewAJ0RrnV"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_42798/708767322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CloK6THIRrnV"
      },
      "source": [
        "## Preprocessing\n",
        "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
        "    * CRF model input format (ex. train.data):\n",
        "        ```\n",
        "        肝 O\n",
        "        功 O\n",
        "        能 O\n",
        "        6 B-med_exam\n",
        "        8 I-med_exam\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def proprocessing(source_path , target_path):\n",
        "\n",
        "    if os.path.isfile(target_path):\n",
        "        os.remove(target_path)\n",
        "\n",
        "    all_datas = []\n",
        "\n",
        "    with open(source_path,'r', encoding='utf8') as f:\n",
        "        article_item_list = f.read().encode('utf-8').decode('utf-8-sig').split('\\n\\n--------------------\\n\\n')[:-1] \n",
        "    \n",
        "    # article_item:  article + items\n",
        "    for article_item in article_item_list:\n",
        "        # split every line in article item\n",
        "        article_item = article_item.split('\\n')\n",
        "        \n",
        "        temp = {\"article\":None , \"id\":None  , \"items\":[]   }\n",
        "        temp[\"article\"] = article_item[0]\n",
        "\n",
        "        for i in range(2,len(article_item)): # start from 2: skip column headers\n",
        "            item = article_item[i].split('\\t')\n",
        "            item[:3] = [int(t) for t in item[:3]]\n",
        "            temp[\"items\"].append(item) \n",
        "        temp[\"id\"] = temp[\"items\"][0][0]\n",
        "        all_datas.append(temp)\n",
        "\n",
        "    print(f\"Theres {len(all_datas)} articles\")\n",
        "\n",
        "    with open(target_path ,'w+') as f:\n",
        "\n",
        "        for data in all_datas:\n",
        "            article_words = [w for w in data[\"article\"]]\n",
        "            labels = ['O'] * len(article_words)\n",
        "            items = data[\"items\"]\n",
        "\n",
        "            for item in items:\n",
        "                labels[item[1]:item[2]] =  [f\"I-{item[-1]}\"] * (item[2]-item[1])\n",
        "                labels[item[1]] = 'B-' + item[-1]\n",
        "\n",
        "            for w,l in zip(article_words,labels):\n",
        "                f.write(f\"{w} {l}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    return all_datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk4hGQu1RrnV",
        "outputId": "c4f27dc0-ec86-42d2-c358-8d4aecdd66ea"
      },
      "outputs": [],
      "source": [
        "SOURCE_TXT = \"./dataset/SampleData_deid.txt\"\n",
        "TARGET_PATH = \"./dataset/data.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_datas = proprocessing(SOURCE_TXT,TARGET_PATH)\n",
        "all_datas[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYBIWnlRrnW"
      },
      "source": [
        "## NER model\n",
        "### CRF (Conditional Random Field model)\n",
        "* Using `sklearn-crfsuite` API\n",
        "\n",
        "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI9UJRfKRrnW",
        "outputId": "cb733fee-1104-4cc3-9903-fcdf3e016fef"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn-crfsuite\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite.metrics import flat_classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMS2hCoRrnW"
      },
      "outputs": [],
      "source": [
        "def CRF(x_train, y_train, x_test, y_test):\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        c1=0.1,\n",
        "        c2=0.1,\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "    crf.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = crf.predict(x_test)\n",
        "    y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "\n",
        "    return y_pred, y_pred_mar, f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhx6pJSHRrnW"
      },
      "source": [
        "## Model Input: \n",
        "* input features:\n",
        "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
        "    \n",
        "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UculQjKRrnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHtTSi_8RrnW"
      },
      "outputs": [],
      "source": [
        "# Load pretrained word vectors\n",
        "# Get a dict of tokens (key) and their pretrained word vectors (value)\n",
        "# Pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
        "dim = 0\n",
        "word_vecs= {}\n",
        "# Open pretrained word vector file\n",
        "with open('./cna.cbow.cwe_p.tar_g.512d.0.txt') as f:\n",
        "    for line in f:\n",
        "        tokens = line.strip().split()\n",
        "\n",
        "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
        "        if len(tokens) == 2:\n",
        "            dim = int(tokens[1])\n",
        "            continue\n",
        "    \n",
        "        word = tokens[0] \n",
        "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
        "        word_vecs[word] = vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey0U_XhpRrnW",
        "outputId": "3fc04bc6-b12e-42d5-9150-e5e9feb0d7b7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RsMn2QRrnW"
      },
      "source": [
        "Here we split data into training dataset and testing dataset,\n",
        "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
        "\n",
        "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySlER2ErRrnW"
      },
      "outputs": [],
      "source": [
        "# Load `train.data` and separate into a list of labeled data of each text\n",
        "# return:\n",
        "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
        "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
        "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
        "from sklearn.model_selection import train_test_split\n",
        "def Dataset(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
        "    data_list, data_list_tmp = list(), list()\n",
        "    article_id_list=list()\n",
        "    idx=0\n",
        "    for row in data:\n",
        "        data_tuple = tuple()\n",
        "        if row == '\\n':\n",
        "            article_id_list.append(idx)\n",
        "            idx+=1\n",
        "            data_list.append(data_list_tmp)\n",
        "            data_list_tmp = []\n",
        "        else:\n",
        "            row = row.strip('\\n').split(' ')\n",
        "            data_tuple = (row[0], row[1])\n",
        "            data_list_tmp.append(data_tuple)\n",
        "    if len(data_list_tmp) != 0:\n",
        "        data_list.append(data_list_tmp)\n",
        "    \n",
        "    # Here we random split data into training dataset and testing dataset\n",
        "    # But you should take `development data` or `test data` as testing data\n",
        "    # At that time, you could just delete this line, \n",
        "    # nd generate data_list of `train data` and data_list of `development/test data` by this function\n",
        "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
        "                                                                                                    article_id_list,\n",
        "                                                                                                    test_size=0.33,\n",
        "                                                                                                    random_state=42)\n",
        "    \n",
        "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3P8vbYGRrnX"
      },
      "outputs": [],
      "source": [
        "# look up word vectors\n",
        "# turn each word into its pretrained word vector\n",
        "# return a list of word vectors corresponding to each token in train.data\n",
        "def Word2Vector(data_list, embedding_dict):\n",
        "    embedding_list = list()\n",
        "\n",
        "    # No Match Word (unknown word) Vector in Embedding\n",
        "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
        "\n",
        "    for idx_list in range(len(data_list)):\n",
        "        embedding_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            key = data_list[idx_list][idx_tuple][0] # token\n",
        "\n",
        "            if key in embedding_dict:\n",
        "                value = embedding_dict[key]\n",
        "            else:\n",
        "                value = unk_vector\n",
        "            embedding_list_tmp.append(value)\n",
        "        embedding_list.append(embedding_list_tmp)\n",
        "        \n",
        "    return embedding_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzmKa42yRrnX"
      },
      "outputs": [],
      "source": [
        "# Input features: pretrained word vectors of each token\n",
        "# Return a list of feature dicts, each feature dict corresponding to each token\n",
        "def Feature(embed_list):\n",
        "    feature_list = list()\n",
        "    for idx_list in range(len(embed_list)):\n",
        "        feature_list_tmp = list()\n",
        "        for idx_tuple in range(len(embed_list[idx_list])):\n",
        "            feature_dict = dict()\n",
        "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
        "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
        "            feature_list_tmp.append(feature_dict)\n",
        "        feature_list.append(feature_list_tmp)\n",
        "\n",
        "    return feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNa4fSkYRrnX"
      },
      "outputs": [],
      "source": [
        "# Get the labels of each tokens in train.data\n",
        "# Return a list of lists of labels\n",
        "def Preprocess(data_list):\n",
        "    label_list = list()\n",
        "    for idx_list in range(len(data_list)):\n",
        "        label_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
        "        label_list.append(label_list_tmp)\n",
        "        \n",
        "    return label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdoz84b0RrnX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_uGdFy3RrnX"
      },
      "outputs": [],
      "source": [
        "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(TARGET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "traindata_article_id_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testdata_article_id_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDc_mtuXRrnX"
      },
      "outputs": [],
      "source": [
        "# Load Word Embedding\n",
        "trainembed_list = Word2Vector(traindata_list, word_vecs)\n",
        "testembed_list = Word2Vector(testdata_list, word_vecs)\n",
        "\n",
        "print(f\"trainembed_list {len(trainembed_list)} testembed_list {len(testembed_list)}\")\n",
        "\n",
        "# CRF - Train Data (Augmentation Data)\n",
        "x_train = Feature(trainembed_list) # 17 * article len\n",
        "y_train = Preprocess(traindata_list) # 17 * article len\n",
        "\n",
        "# CRF - Test Data (Golden Standard)\n",
        "x_test = Feature(testembed_list) # 9*article len\n",
        "y_test = Preprocess(testdata_list) # 9*article len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbhes9eBRrnX"
      },
      "outputs": [],
      "source": [
        "y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H02UitQBRrnX",
        "outputId": "49fe4afc-1837-4bea-dae5-993a967d9199",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fasttext + find best parameters of CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "# load fast text model\n",
        "model = fasttext.load_model('./dataset/cc.zh.300.bin')\n",
        "model.get_dimension()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_to_feature(embedding):\n",
        "    return { str(\"dim_\") + str(i):e for i,e in enumerate(embedding) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding_label( data , fasttext_model): # given list of ('w',label) , return crf input x and label y\n",
        "    \n",
        "    x = [ [] for i in range(len(data))]\n",
        "    y = [ [] for i in range(len(data))]\n",
        "\n",
        "    for article_idx , article in enumerate(data):\n",
        "        x[article_idx] =  [  embedding_to_feature(fasttext_model[i[0]]) for i in  article]\n",
        "        y[article_idx] = [ i[-1] for i in article]\n",
        "\n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train , y_train = get_embedding_label( traindata_list , model )\n",
        "x_test , y_test = get_embedding_label( testdata_list , model )\n",
        "\n",
        "print(f\" train_x {len(x_train)} train_y {len(y_train)} \")\n",
        "print(f\" test_x {len(x_test)} test_y {len(y_test)} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test(x_train,y_train,x_test,y_test , c1 =0.1, c2=0.1):\n",
        "\n",
        "    # union all labels\n",
        "    labels = set([item for sublist in y_train for item in sublist]) | set([item for sublist in y_test for item in sublist]) \n",
        "\n",
        "    # crf model\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True,\n",
        "        c1=c1,\n",
        "        c2=c2,\n",
        "    )\n",
        "    # fit\n",
        "    crf.fit(x_train, y_train)\n",
        "    # pred\n",
        "    y_pred = crf.predict(x_test)\n",
        "    y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "    \n",
        "    return y_pred, y_pred_mar, f1score , crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# find best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exp_result = []\n",
        "best_f1 = [-1,0,0]\n",
        "\n",
        "for c1 in np.arange(0.001,0.01,0.003):\n",
        "    for c2 in (0.001,0.01,0.003):\n",
        "        y_pred, y_pred_mar, f1score , crf_model = train_test(x_train, y_train, x_test, y_test,c1,c2)\n",
        "        if best_f1[0] < f1score:\n",
        "            best_f1 = f1score , c1,c2            \n",
        "        exp_result.append([c1, c2, f1score])\n",
        "\n",
        "print(f\"Best f1 score is {best_f1[0]} c1 {best_f1[1]} c2 {best_f1[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = pd.DataFrame(columns=['c1','c2','f1-score'])\n",
        "x[['c1','c2','f1-score']] = exp_result\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0nOItHSRrnX"
      },
      "source": [
        "## Output data\n",
        "* Change model output into `output.tsv` \n",
        "* Only accept this output format uploading to competition system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC66RB5tRrnX"
      },
      "outputs": [],
      "source": [
        "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
        "for test_id in range(len(y_pred)):\n",
        "    pos=0\n",
        "    start_pos=None\n",
        "    end_pos=None\n",
        "    entity_text=None\n",
        "    entity_type=None\n",
        "    for pred_id in range(len(y_pred[test_id])):\n",
        "        if y_pred[test_id][pred_id][0]=='B':\n",
        "            start_pos=pos\n",
        "            entity_type=y_pred[test_id][pred_id][2:]\n",
        "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n",
        "            end_pos=pos\n",
        "            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
        "            line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
        "            output+=line+'\\n'\n",
        "        pos+=1     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40HnTWGtRrnX"
      },
      "outputs": [],
      "source": [
        "output_path='output.tsv'\n",
        "with open(output_path,'w',encoding='utf-8') as f:\n",
        "    f.write(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M36pQhxRrnX",
        "outputId": "746511dd-acad-4814-f46d-eaf923b9a9a1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36oJuO06RrnX"
      },
      "source": [
        "## Note\n",
        "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
        "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
        "* You may try `CRF++` tool for NER tagging by CRF model\n",
        "    * [Documentation](http://taku910.github.io/crfpp/)\n",
        "    * Need design feature template\n",
        "    * Can only computed in CPU\n",
        "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
        "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
        "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuPLXj8RrnX"
      },
      "source": [
        "-----------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NER_baseline.ipynb",
      "provenance": []
    },
    "file_extension": ".py",
    "interpreter": {
      "hash": "bc9aca527163a64f35853ea59b2f0f783e0ca5aaf51400ee245f68d6b9bfb088"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
