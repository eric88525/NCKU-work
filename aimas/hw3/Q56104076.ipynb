{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xAUewAJ0RrnV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CloK6THIRrnV"
      },
      "source": [
        "## Preprocessing\n",
        "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
        "    * CRF model input format (ex. train.data):\n",
        "        ```\n",
        "        肝 O\n",
        "        功 O\n",
        "        能 O\n",
        "        6 B-med_exam\n",
        "        8 I-med_exam\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def proprocessing(source_path , target_path):\n",
        "\n",
        "    if os.path.isfile(target_path):\n",
        "        os.remove(target_path)\n",
        "\n",
        "    all_datas = []\n",
        "\n",
        "    with open(source_path,'r', encoding='utf8') as f:\n",
        "        article_item_list = f.read().encode('utf-8').decode('utf-8-sig').split('\\n\\n--------------------\\n\\n')[:-1] \n",
        "    \n",
        "    # article_item:  article + items\n",
        "    for article_item in article_item_list:\n",
        "        # split every line in article item\n",
        "        article_item = article_item.split('\\n')\n",
        "        \n",
        "        temp = {\"article\":None , \"id\":None  , \"items\":[]   }\n",
        "        temp[\"article\"] = article_item[0]\n",
        "\n",
        "        for i in range(2,len(article_item)): # start from 2: skip column headers\n",
        "            item = article_item[i].split('\\t')\n",
        "            item[:3] = [int(t) for t in item[:3]]\n",
        "            temp[\"items\"].append(item) \n",
        "        temp[\"id\"] = temp[\"items\"][0][0]\n",
        "        all_datas.append(temp)\n",
        "\n",
        "    print(f\"Theres {len(all_datas)} articles\")\n",
        "\n",
        "    with open(target_path ,'w+') as f:\n",
        "\n",
        "        for data in all_datas:\n",
        "            article_words = [w for w in data[\"article\"]]\n",
        "            labels = ['O'] * len(article_words)\n",
        "            items = data[\"items\"]\n",
        "\n",
        "            for item in items:\n",
        "                labels[item[1]:item[2]] =  [f\"I-{item[-1]}\"] * (item[2]-item[1])\n",
        "                labels[item[1]] = 'B-' + item[-1]\n",
        "\n",
        "            for w,l in zip(article_words,labels):\n",
        "                f.write(f\"{w} {l}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    return all_datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nk4hGQu1RrnV",
        "outputId": "c4f27dc0-ec86-42d2-c358-8d4aecdd66ea"
      },
      "outputs": [],
      "source": [
        "SOURCE_TXT = \"./dataset/SampleData_deid.txt\"\n",
        "TARGET_PATH = \"./dataset/data.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theres 26 articles\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'article': '醫師：阿嬤回去狀況怎麽樣？家屬：這個就是她出院的時候有開軟便藥，DRN藥。啊，我們出院當天開始配睡前，兩顆。醫師：兩顆。家屬：那，那個住院後有，還是有消化差的情況，目前是牛奶加水，100ml每餐。醫師：100，牛奶加水，100ml每餐。家屬：對。醫師：那加起來的話會到幾ml？家屬：就牛奶加水，就100ml，一天7餐。醫師：7餐，那這樣一天才700哦。變，變差。家屬：那可是還，還是會有50ml的會消化不良。醫師：真的哦？家屬：對。那，目前使用灌食袋這個部分的情況就請醫師那個，評估一下。醫師：好。家屬：第三個注明她有吃這類wakamoto。醫師：wakamoto哦？家屬：欸9顆，3餐飯後使用。那這個可以并服？醫師：可以哦，可以并服哦。家屬：可哦。好，那接下來作業期間她有追蹤尿液的細菌培養說有桿菌，那請問醫師這個部分需要做接觸性的隔離嗎？醫師：啊，不用，就一般性的就好了。家屬：一般性。醫師：你要處理她的尿布啊。你就掛手套處理啦。家屬：就一般性隔離。醫師：就可以了。這個在醫院我們也沒有在隔離啦。家屬：好。接觸尿布的時候帶手套？醫師：就是接觸體液，比方説，她的大小便啊，還是抽痰的時候，照顧的人還是戴一下手套這樣子。就是一般的，就是一般標準就可以了。家屬：好。那出院之後，沒有發燒，生命徵象也穩定。醫師：也穩定。家屬：也沒有抽痰。醫師：好。家屬：對。那剛來門診之前，她有做那個抽血。醫師：抽血的檢查。家屬：對。醫師：哦，報告還沒有出來這樣子。家屬：對，應該不會那麽快吧？醫師：對，不會那麽快。家屬：我抽完這樣，就直接過來這裏了。醫師：哦。家屬：那也只能下次再看看。醫師：在看看囉。我幫她聽一下。她消化的部分本來，本來在醫院還可以勒，後來反而變，在出院之後比較差了一點。家屬：對，那這樣子變成說一餐100ml還有50ml的那個未消化就很差啦。而且我們也是用灌食帶這個速度。醫師：你們也是用灌食袋這樣？家屬：對，目前是使用灌食袋，速度上也應該不至於說太快這樣。醫師：太快這樣。家屬：對。醫師：我們也有再開那個促進腸胃蠕動的。家屬：好。醫師：好像，有沒有脹氣。家屬：有脹氣嗎？醫師：一點點，我開一點，再開點脹氣，然後腸胃蠕動的再藥繼續，繼續吃好不好？家屬：好。醫師：便便也都還好哦？家屬：對，因爲目前是，那個睡前兩顆，有計時，有固定配了。醫師：好。那我，我依序給她，就是你們護理之家那邊都給她回答這樣子。家屬：好，謝謝。醫師：首爾，她住在首爾，阿首爾護理之家是在哪裏？家屬：跟釜山同一個地點。醫師：釜山，啊釜山在哪裏？家屬：南韓交流道附近。醫師：南韓交流道，你是，你都是固定在哪裏嗎？家屬：我是裏面，算是裏面的員工，裏面的代診人員。醫師：是是。所以你都固定在那一間做很久了，這樣子？還是你兼好幾個？家屬：Allinone。醫師：Allinone，好。家屬：我不是周遊列國。醫師：你不是哦……那現在就是消化這樣？家屬：對，因爲她這個消化的問題很久了。醫師：很久了哦。家屬：很久了，那從診所一直到醫院，估計至少有六個月以上，甚至到一年也有可能。醫師：好，那個尿道感染因爲我們已經有治療過，那細菌已經消失，所以是ok。這樣可能還要再，我還要再追蹤一下這樣子。家屬：再追蹤。醫師：對對。家屬：血液？醫師：嗯。家屬：那這樣子……醫師：下次抽血，下次看抽血的報告這樣子。家屬：下次看今天的抽血報告？醫師：對，對。家屬：那下次門診之前需要再抽血嗎？醫師：先不用好不好？我想先約三個禮拜。家屬：好。那看報告而已，那她本人就不用來？醫師：對，可以。但是另外她有一個在檢查，在醫院的檢查報告就是，她的指數比較高一點。家屬：什麽指數？醫師：自體免疫指數比較高。就是，這個我會再追蹤好不好？因爲有個ANA指數這樣子。不過她已經高齡了，我想，這個我會再注意看看，她那個，她的自體指數，自體免疫的指數比較高哦。我看，看會不會影響到她的腎功能這些。家屬：好。醫師：這個有需要我會再請我的同事幫忙看一下這樣子。家屬：好，那就不用再排什麽檢查之類的？醫師：對，因爲再做的一些檢查，因爲她現在人好好的，所以先不用啦，我覺得這樣就，先觀察就好了。那我們先約三個禮拜，假如狀況都還好，阿嬤就先不用回來好不好。家屬：好。醫師：三個禮拜的話是6月20號。家屬：好。',\n",
              " 'id': 5,\n",
              " 'items': [[5, 1012, 1014, '首爾', 'location'],\n",
              "  [5, 1018, 1020, '首爾', 'location'],\n",
              "  [5, 1022, 1028, '首爾護理之家', 'location'],\n",
              "  [5, 1037, 1039, '釜山', 'location'],\n",
              "  [5, 1048, 1050, '釜山', 'location'],\n",
              "  [5, 1052, 1054, '釜山', 'location'],\n",
              "  [5, 1061, 1066, '南韓交流道', 'location'],\n",
              "  [5, 1072, 1077, '南韓交流道', 'location'],\n",
              "  [5, 1253, 1256, '六個月', 'time'],\n",
              "  [5, 1262, 1264, '一年', 'time'],\n",
              "  [5, 1430, 1434, '三個禮拜', 'time'],\n",
              "  [5, 1708, 1712, '三個禮拜', 'time'],\n",
              "  [5, 1741, 1745, '三個禮拜', 'time'],\n",
              "  [5, 1748, 1753, '6月20號', 'time']]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_datas = proprocessing(SOURCE_TXT,TARGET_PATH)\n",
        "all_datas[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYBIWnlRrnW"
      },
      "source": [
        "## NER model\n",
        "### CRF (Conditional Random Field model)\n",
        "* Using `sklearn-crfsuite` API\n",
        "\n",
        "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI9UJRfKRrnW",
        "outputId": "cb733fee-1104-4cc3-9903-fcdf3e016fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn-crfsuite in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (0.3.6)\n",
            "Requirement already satisfied: tabulate in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from sklearn-crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from sklearn-crfsuite) (4.62.3)\n",
            "Requirement already satisfied: six in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from sklearn-crfsuite) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn-crfsuite\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite.metrics import flat_classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6GMS2hCoRrnW"
      },
      "outputs": [],
      "source": [
        "def CRF(x_train, y_train, x_test, y_test):\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        c1=0.1,\n",
        "        c2=0.1,\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "    crf.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = crf.predict(x_test)\n",
        "    y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "\n",
        "    return y_pred, y_pred_mar, f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhx6pJSHRrnW"
      },
      "source": [
        "## Model Input: \n",
        "* input features:\n",
        "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
        "    \n",
        "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7UculQjKRrnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tHtTSi_8RrnW"
      },
      "outputs": [],
      "source": [
        "# Load pretrained word vectors\n",
        "# Get a dict of tokens (key) and their pretrained word vectors (value)\n",
        "# Pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
        "dim = 0\n",
        "word_vecs= {}\n",
        "# Open pretrained word vector file\n",
        "with open('./cna.cbow.cwe_p.tar_g.512d.0.txt') as f:\n",
        "    for line in f:\n",
        "        tokens = line.strip().split()\n",
        "\n",
        "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
        "        if len(tokens) == 2:\n",
        "            dim = int(tokens[1])\n",
        "            continue\n",
        "    \n",
        "        word = tokens[0] \n",
        "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
        "        word_vecs[word] = vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey0U_XhpRrnW",
        "outputId": "3fc04bc6-b12e-42d5-9150-e5e9feb0d7b7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary_size:  158566  word_vector_dim:  (512,)\n"
          ]
        }
      ],
      "source": [
        "print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RsMn2QRrnW"
      },
      "source": [
        "Here we split data into training dataset and testing dataset,\n",
        "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
        "\n",
        "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ySlER2ErRrnW"
      },
      "outputs": [],
      "source": [
        "# Load `train.data` and separate into a list of labeled data of each text\n",
        "# return:\n",
        "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
        "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
        "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
        "from sklearn.model_selection import train_test_split\n",
        "def Dataset(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
        "    data_list, data_list_tmp = list(), list()\n",
        "    article_id_list=list()\n",
        "    idx=0\n",
        "    for row in data:\n",
        "        data_tuple = tuple()\n",
        "        if row == '\\n':\n",
        "            article_id_list.append(idx)\n",
        "            idx+=1\n",
        "            data_list.append(data_list_tmp)\n",
        "            data_list_tmp = []\n",
        "        else:\n",
        "            row = row.strip('\\n').split(' ')\n",
        "            data_tuple = (row[0], row[1])\n",
        "            data_list_tmp.append(data_tuple)\n",
        "    if len(data_list_tmp) != 0:\n",
        "        data_list.append(data_list_tmp)\n",
        "    \n",
        "    # Here we random split data into training dataset and testing dataset\n",
        "    # But you should take `development data` or `test data` as testing data\n",
        "    # At that time, you could just delete this line, \n",
        "    # nd generate data_list of `train data` and data_list of `development/test data` by this function\n",
        "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
        "                                                                                                    article_id_list,\n",
        "                                                                                                    test_size=0.33,\n",
        "                                                                                                    random_state=42)\n",
        "    \n",
        "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T3P8vbYGRrnX"
      },
      "outputs": [],
      "source": [
        "# look up word vectors\n",
        "# turn each word into its pretrained word vector\n",
        "# return a list of word vectors corresponding to each token in train.data\n",
        "def Word2Vector(data_list, embedding_dict):\n",
        "    embedding_list = list()\n",
        "\n",
        "    # No Match Word (unknown word) Vector in Embedding\n",
        "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
        "\n",
        "    for idx_list in range(len(data_list)):\n",
        "        embedding_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            key = data_list[idx_list][idx_tuple][0] # token\n",
        "\n",
        "            if key in embedding_dict:\n",
        "                value = embedding_dict[key]\n",
        "            else:\n",
        "                value = unk_vector\n",
        "            embedding_list_tmp.append(value)\n",
        "        embedding_list.append(embedding_list_tmp)\n",
        "        \n",
        "    return embedding_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qzmKa42yRrnX"
      },
      "outputs": [],
      "source": [
        "# Input features: pretrained word vectors of each token\n",
        "# Return a list of feature dicts, each feature dict corresponding to each token\n",
        "def Feature(embed_list):\n",
        "    feature_list = list()\n",
        "    for idx_list in range(len(embed_list)):\n",
        "        feature_list_tmp = list()\n",
        "        for idx_tuple in range(len(embed_list[idx_list])):\n",
        "            feature_dict = dict()\n",
        "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
        "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
        "            feature_list_tmp.append(feature_dict)\n",
        "        feature_list.append(feature_list_tmp)\n",
        "\n",
        "    return feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dNa4fSkYRrnX"
      },
      "outputs": [],
      "source": [
        "# Get the labels of each tokens in train.data\n",
        "# Return a list of lists of labels\n",
        "def Preprocess(data_list):\n",
        "    label_list = list()\n",
        "    for idx_list in range(len(data_list)):\n",
        "        label_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
        "        label_list.append(label_list_tmp)\n",
        "        \n",
        "    return label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdoz84b0RrnX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e_uGdFy3RrnX"
      },
      "outputs": [],
      "source": [
        "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(TARGET_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[5, 2, 12, 15, 3, 4, 21, 17, 22, 18, 25, 20, 7, 10, 14, 19, 6]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "traindata_article_id_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[8, 16, 0, 24, 11, 9, 13, 1, 23]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testdata_article_id_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BDc_mtuXRrnX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainembed_list 17 testembed_list 9\n"
          ]
        }
      ],
      "source": [
        "# Load Word Embedding\n",
        "trainembed_list = Word2Vector(traindata_list, word_vecs)\n",
        "testembed_list = Word2Vector(testdata_list, word_vecs)\n",
        "\n",
        "print(f\"trainembed_list {len(trainembed_list)} testembed_list {len(testembed_list)}\")\n",
        "\n",
        "# CRF - Train Data (Augmentation Data)\n",
        "x_train = Feature(trainembed_list) # 17 * article len\n",
        "y_train = Preprocess(traindata_list) # 17 * article len\n",
        "\n",
        "# CRF - Test Data (Golden Standard)\n",
        "x_test = Feature(testembed_list) # 9*article len\n",
        "y_test = Preprocess(testdata_list) # 9*article len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hbhes9eBRrnX"
      },
      "outputs": [],
      "source": [
        "y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H02UitQBRrnX",
        "outputId": "49fe4afc-1837-4bea-dae5-993a967d9199",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3718508791311577"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# fasttext + find best parameters of CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from fasttext) (1.22.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from fasttext) (2.9.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /home/q56104076/.local/share/virtualenvs/hw3-fLRQMIK9/lib/python3.8/site-packages (from fasttext) (44.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fast text model:  \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.bin.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "# load fast text model\n",
        "model = fasttext.load_model('./dataset/cc.zh.300.bin')\n",
        "model.get_dimension()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_to_feature(embedding):\n",
        "    return { str(\"dim_\") + str(i):e for i,e in enumerate(embedding) }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding_label( data , fasttext_model): # given list of ('w',label) , return crf input x and label y\n",
        "    \n",
        "    x = [ [] for i in range(len(data))]\n",
        "    y = [ [] for i in range(len(data))]\n",
        "\n",
        "    for article_idx , article in enumerate(data):\n",
        "        x[article_idx] =  [  embedding_to_feature(fasttext_model[i[0]]) for i in  article]\n",
        "        y[article_idx] = [ i[-1] for i in article]\n",
        "\n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " train_x 17 train_y 17 \n",
            " test_x 9 test_y 9 \n"
          ]
        }
      ],
      "source": [
        "x_train , y_train = get_embedding_label( traindata_list , model )\n",
        "x_test , y_test = get_embedding_label( testdata_list , model )\n",
        "\n",
        "print(f\" train_x {len(x_train)} train_y {len(y_train)} \")\n",
        "print(f\" test_x {len(x_test)} test_y {len(y_test)} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test(x_train,y_train,x_test,y_test , c1 =0.1, c2=0.1):\n",
        "\n",
        "    # union all labels\n",
        "    labels = set([item for sublist in y_train for item in sublist]) | set([item for sublist in y_test for item in sublist]) \n",
        "\n",
        "    # crf model\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True,\n",
        "        c1=c1,\n",
        "        c2=c2,\n",
        "    )\n",
        "    # fit\n",
        "    crf.fit(x_train, y_train)\n",
        "    # pred\n",
        "    y_pred = crf.predict(x_test)\n",
        "    y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "    \n",
        "    return y_pred, y_pred_mar, f1score , crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# find best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best f1 score is 0.6446313390506472 c1 0.007 c2 0.010000000000000002\n"
          ]
        }
      ],
      "source": [
        "exp_result = []\n",
        "best_f1 = [-1,0,0]\n",
        "\n",
        "for c1 in np.arange(0.001,0.01,0.003):\n",
        "    for c2 in np.arange(0.001,0.01,0.003):\n",
        "        y_pred, y_pred_mar, f1score , crf_model = train_test(x_train, y_train, x_test, y_test,c1,c2)\n",
        "        if best_f1[0] < f1score:\n",
        "            best_f1 = f1score , c1,c2            \n",
        "        exp_result.append([c1, c2, f1score])\n",
        "\n",
        "print(f\"Best f1 score is {best_f1[0]} c1 {best_f1[1]} c2 {best_f1[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.572132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.600787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.623694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.606201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.596468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.598542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.599507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.004</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.611910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.612683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.608164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.613567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.007</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.644631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.595744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.609274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.624091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.010</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.637735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       c1     c2  f1-score\n",
              "0   0.001  0.001  0.572132\n",
              "1   0.001  0.004  0.600787\n",
              "2   0.001  0.007  0.623694\n",
              "3   0.001  0.010  0.606201\n",
              "4   0.004  0.001  0.596468\n",
              "5   0.004  0.004  0.598542\n",
              "6   0.004  0.007  0.599507\n",
              "7   0.004  0.010  0.611910\n",
              "8   0.007  0.001  0.612683\n",
              "9   0.007  0.004  0.608164\n",
              "10  0.007  0.007  0.613567\n",
              "11  0.007  0.010  0.644631\n",
              "12  0.010  0.001  0.595744\n",
              "13  0.010  0.004  0.609274\n",
              "14  0.010  0.007  0.624091\n",
              "15  0.010  0.010  0.637735"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = pd.DataFrame(columns=['c1','c2','f1-score'])\n",
        "x[['c1','c2','f1-score']] = exp_result\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0nOItHSRrnX"
      },
      "source": [
        "## Output data\n",
        "* Change model output into `output.tsv` \n",
        "* Only accept this output format uploading to competition system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC66RB5tRrnX"
      },
      "outputs": [],
      "source": [
        "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
        "for test_id in range(len(y_pred)):\n",
        "    pos=0\n",
        "    start_pos=None\n",
        "    end_pos=None\n",
        "    entity_text=None\n",
        "    entity_type=None\n",
        "    for pred_id in range(len(y_pred[test_id])):\n",
        "        if y_pred[test_id][pred_id][0]=='B':\n",
        "            start_pos=pos\n",
        "            entity_type=y_pred[test_id][pred_id][2:]\n",
        "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n",
        "            end_pos=pos\n",
        "            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
        "            line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
        "            output+=line+'\\n'\n",
        "        pos+=1     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40HnTWGtRrnX"
      },
      "outputs": [],
      "source": [
        "output_path='output.tsv'\n",
        "with open(output_path,'w',encoding='utf-8') as f:\n",
        "    f.write(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M36pQhxRrnX",
        "outputId": "746511dd-acad-4814-f46d-eaf923b9a9a1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36oJuO06RrnX"
      },
      "source": [
        "## Note\n",
        "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
        "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
        "* You may try `CRF++` tool for NER tagging by CRF model\n",
        "    * [Documentation](http://taku910.github.io/crfpp/)\n",
        "    * Need design feature template\n",
        "    * Can only computed in CPU\n",
        "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
        "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
        "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuPLXj8RrnX"
      },
      "source": [
        "-----------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NER_baseline.ipynb",
      "provenance": []
    },
    "file_extension": ".py",
    "interpreter": {
      "hash": "bc9aca527163a64f35853ea59b2f0f783e0ca5aaf51400ee245f68d6b9bfb088"
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
